# ===== Canonical pretrained compatibility (must match your weights) =====
data_path: "../data/experimental/sample_train.h5"   # REQUIRED for mode=dataset; else may be null
image_shape: [1, 64, 64, 64]
attributes:
  - ABS_f_D
  - CT_f_D_tort1
  - CT_f_A_tort1

vae:
  latent_dim_channels: 1
  kld_loss_weight: 0.000001
  max_epochs: 0
  pretrained_path: "../models/weights/experimental/vae.pt"
  first_layer_downsample: true
  max_channels: 512

fp:
  max_epochs: 0
  pretrained_path: "../models/weights/experimental/fp.pt"

ddpm:
  timesteps: 1000
  n_feat: 512
  max_epochs: 0
  pretrained_path: "../models/weights/experimental/ddpm.pt"
  context_attributes:        # the conditioning order used for DDPM
    - ABS_f_D
    - CT_f_D_tort1
    - CT_f_A_tort1

# ===== Inference controls =====
inference:
  mode: "dataset"           # one of: "constant" | "random" | "dataset"
  total_samples: 20         # total number of samples to generate
  batch_size: 10           # batch size used during generation

  # mode = constant
  constant_context:          # one row; order can be full attributes OR context_attributes
    - 0.5
    - 0.2
    - 0.2

  # mode = random
  random:
    ranges:                  # define ranges ONLY for context_attributes
      ABS_f_D: [0.4, 0.6]
      CT_f_D_tort1: [0.15, 0.35]
      CT_f_A_tort1: [0.15, 0.35]

  # mode = dataset
  dataset_loader: "train"      # "train" or "val". use train if you use validation dataset file in data_path

# ===== Output controls =====
output:
  output_dir: "./output"     # base directory
  write_vti: true
  write_csv: true
  threshold: 0.5
  save_every_batch: true     # write CSV incrementally each batch (large runs safety)
  csv_inputs: "inputs_context.csv"
  csv_outputs: "outputs_predicted.csv"
